{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m9cJCQti-glA",
        "outputId": "64929315-674a-4d1c-d0c0-2ad6719ecc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: lightrag[ollama] in /usr/local/lib/python3.11/dist-packages (0.1.0b6)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (3.1.6)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (4.0.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.26.4)\n",
            "Requirement already satisfied: ollama<0.3.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (0.2.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.1.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (4.67.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (3.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (25.3.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.13.2)\n",
            "Loading MunshiNagarData.geojson...\n",
            "Loaded GeoJSON with 212 features\n",
            "Running basic anomaly detection...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**GeoJSON Anomaly Analysis:**\n\nBased on the provided GeoJSON data and statistical analysis, I've identified several potential issues:\n\n**Geometric Anomalies**\n\n1. **Self-intersection error**: Features 0, 2 have self-intersecting polygons, which can be a sign of invalid geometry.\n\t* Feature 0: Geometry parsing error at [72.8359824775752, 19.1220712286071]\n\t* Feature 2: Geometry parsing error at [72.8331855051968, 19.1223147076067]\n\n**Statistical Outliers**\n\n2. **Longitude outliers**: Index 55 has an unusual longitude value (75.3622) compared to the mean (72.8404).\n\t* Feature ID: 55\n\t* Longitude: 75.3622\n\n3. **Latitude outliers**: Index 55 has a latitude value (90.65) that's significantly higher than the mean (19.1637) and maximum allowed value (90).\n\n**Spatial Outliers**\n\n4. **Invalid location**: Feature ID: 55\n\t* Latitude: 90.65 (outside -90 to 90 range)\n\t* Longitude: 75.3622\n\n**Data Consistency Issues**\n\n5. **Missing data**: No numpy.rec module found during analysis, which might indicate missing or incomplete libraries in the environment.\n\n**Additional Findings**\n\n6. **Invalid coordinates**: Several features have invalid latitude values outside the -90 to 90 range.\n\t* Notably: Features with IDs 55 and others (not specified) have latitude values outside this range.\n\nTo address these issues, I recommend:\n\n1. Re-examine the GeoJSON data for geometric validity and correct any self-intersection errors.\n2. Verify the coordinate accuracy of features with outliers (e.g., feature ID 55).\n3. Review the libraries and environment to ensure numpy.rec is properly installed.\n4. Investigate missing or invalid latitude values and correct them accordingly.\n\nHere's a summary of the findings in GeoJSON format:\n```json\n{\n  \"issues\": [\n    {\n      \"feature_id\": 0,\n      \"error_type\": \"geometry_parsing_error\",\n      \"coordinates\": [72.8359824775752, 19.1220712286071]\n    },\n    {\n      \"feature_id\": 2,\n      \"error_type\": \"self_intersection\",\n      \"coordinates\": [72.8331855051968, 19.1223147076067]\n    },\n    {\n      \"feature_id\": 55,\n      \"error_type\": \"longitude_outlier\",\n      \"longitude\": 75.3622\n    },\n    {\n      \"feature_id\": 55,\n      \"error_type\": \"latitude_outlier\",\n      \"latitude\": 90.65\n    },\n    {\n      \"error_type\": \"missing_data\",\n      \"library\": \"numpy.rec\"\n    }\n  ]\n}\n```\nThese findings should help you identify and address the potential issues in your GeoJSON data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Running detailed analysis focusing on geographic and property anomalies...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**GeoJSON Anomaly Analysis:**\n\nTo detect anomalies in the provided GeoJSON data, I'll perform the following analysis:\n\n**1. Geometric Anomalies**\n\n*   Feature 0 has a geometry parsing error due to an invalid 'float' object being iterable.\n    *   Suggested action: Validate the input data for correct data types.\n\n*   Features 1 and 2 have self-intersections, indicating geometric anomalies.\n    *   Coordinates of these features:\n        -   Feature 1: [72.8359824775752, 19.1220712286071]\n        -   Feature 2: [72.8331855051968, 19.1223147076067]\n\n**2. Statistical Outliers in Properties/Attributes**\n\n*   Longitude outliers detected at index 55.\n    *   Coordinate of the feature with an outlier longitude value:\n        -   Index 55: [75.3622, latitude\\_value] (Note: The exact coordinate is not provided due to missing data)\n\n*   Latitude outliers detected at index 55.\n    *   Coordinate of the feature with an outlier latitude value:\n        -   Index 55: [longitude\\_value, 90.65] (Note: The exact coordinate is not provided due to missing data)\n\n**3. Spatial Outliers**\n\n*   Invalid latitude values found outside the -90 to 90 range.\n    *   Feature IDs where invalid latitude values are detected:\n        -   Features with latitude value greater than 90\n        -   Features with latitude value less than -90\n\n**4. Data Consistency Issues**\n\n*   Analysis error due to a missing module 'numpy.rec'\n    *   Suggested action: Install the required numpy module for analysis.\n\n**5. Missing or Malformed Data**\n\n*   Invalid latitude values found outside the -90 to 90 range.\n    *   Feature IDs where invalid latitude values are detected:\n        -   Features with latitude value greater than 90\n        -   Features with latitude value less than -90"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeoJSON Anomaly Detection system is ready!\n",
            "Use detect_geojson_anomalies(your_geojson_data) to analyze your data.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "!ollama pull llama3.1:8b\n",
        "clear_output()\n",
        "!pip install -U lightrag[ollama] geopy shapely\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from geopy.distance import geodesic\n",
        "from shapely.geometry import Point, Polygon, LineString, shape\n",
        "from shapely.validation import explain_validity\n",
        "import time\n",
        "\n",
        "# Enhanced template for GeoJSON anomaly detection\n",
        "geojson_anomaly_template = r\"\"\"<SYS>\n",
        "You are an expert geospatial data analyst specializing in detecting anomalies in GeoJSON data.\n",
        "Analyze the provided GeoJSON data and statistical analysis to identify potential anomalies.\n",
        "\n",
        "Focus on detecting:\n",
        "1. Geometric anomalies (invalid coordinates, self-intersecting polygons, etc.)\n",
        "2. Statistical outliers in properties/attributes\n",
        "3. Spatial outliers (features in unexpected locations)\n",
        "4. Data consistency issues\n",
        "5. Missing or malformed data\n",
        "\n",
        "Provide specific, actionable findings with coordinates and feature IDs where applicable.\n",
        "</SYS>\n",
        "\n",
        "Statistical Analysis:\n",
        "{{stats}}\n",
        "\n",
        "GeoJSON Data Summary:\n",
        "{{geojson_summary}}\n",
        "\n",
        "Detected Issues:\n",
        "{{detected_issues}}\n",
        "\n",
        "User Query: {{input_str}}\n",
        "\n",
        "Analysis:\"\"\"\n",
        "\n",
        "class GeoJSONAnomalyDetector(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=geojson_anomaly_template,\n",
        "        )\n",
        "\n",
        "    def analyze_geojson(self, geojson_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Perform statistical and geometric analysis on GeoJSON data\"\"\"\n",
        "        issues = []\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            features = geojson_data.get('features', [])\n",
        "            stats['total_features'] = len(features)\n",
        "\n",
        "            # Analyze geometric properties\n",
        "            coordinates_list = []\n",
        "            properties_data = []\n",
        "            geometry_types = []\n",
        "\n",
        "            for i, feature in enumerate(features):\n",
        "                # Geometry analysis\n",
        "                geometry = feature.get('geometry', {})\n",
        "                geom_type = geometry.get('type', '')\n",
        "                geometry_types.append(geom_type)\n",
        "\n",
        "                # Validate geometry using Shapely\n",
        "                try:\n",
        "                    shape_obj = shape(geometry)\n",
        "                    if not shape_obj.is_valid:\n",
        "                        issues.append(f\"Invalid geometry in feature {i}: {explain_validity(shape_obj)}\")\n",
        "                except Exception as e:\n",
        "                    issues.append(f\"Geometry parsing error in feature {i}: {str(e)}\")\n",
        "\n",
        "                # Extract coordinates for analysis\n",
        "                coords = geometry.get('coordinates', [])\n",
        "                if coords:\n",
        "                    flat_coords = self._flatten_coordinates(coords)\n",
        "                    coordinates_list.extend(flat_coords)\n",
        "\n",
        "                # Properties analysis\n",
        "                properties = feature.get('properties', {})\n",
        "                properties_data.append(properties)\n",
        "\n",
        "            # Statistical analysis of coordinates\n",
        "            if coordinates_list:\n",
        "                coords_array = np.array(coordinates_list)\n",
        "                if coords_array.shape[1] >= 2:\n",
        "                    lons = coords_array[:, 0]\n",
        "                    lats = coords_array[:, 1]\n",
        "\n",
        "                    stats['longitude'] = {\n",
        "                        'min': float(np.min(lons)),\n",
        "                        'max': float(np.max(lons)),\n",
        "                        'mean': float(np.mean(lons)),\n",
        "                        'std': float(np.std(lons))\n",
        "                    }\n",
        "                    stats['latitude'] = {\n",
        "                        'min': float(np.min(lats)),\n",
        "                        'max': float(np.max(lats)),\n",
        "                        'mean': float(np.mean(lats)),\n",
        "                        'std': float(np.std(lats))\n",
        "                    }\n",
        "\n",
        "                    # Detect coordinate outliers\n",
        "                    lon_outliers = np.abs(lons - np.mean(lons)) > 3 * np.std(lons)\n",
        "                    lat_outliers = np.abs(lats - np.mean(lats)) > 3 * np.std(lats)\n",
        "\n",
        "                    if np.any(lon_outliers):\n",
        "                        outlier_indices = np.where(lon_outliers)[0]\n",
        "                        issues.append(f\"Longitude outliers detected at indices: {outlier_indices.tolist()}\")\n",
        "\n",
        "                    if np.any(lat_outliers):\n",
        "                        outlier_indices = np.where(lat_outliers)[0]\n",
        "                        issues.append(f\"Latitude outliers detected at indices: {outlier_indices.tolist()}\")\n",
        "\n",
        "                    # Check for invalid coordinate ranges\n",
        "                    if np.any(lons < -180) or np.any(lons > 180):\n",
        "                        issues.append(\"Invalid longitude values found (outside -180 to 180 range)\")\n",
        "\n",
        "                    if np.any(lats < -90) or np.any(lats > 90):\n",
        "                        issues.append(\"Invalid latitude values found (outside -90 to 90 range)\")\n",
        "\n",
        "            # Analyze geometry types\n",
        "            stats['geometry_types'] = dict(pd.Series(geometry_types).value_counts())\n",
        "\n",
        "            # Analyze properties\n",
        "            if properties_data:\n",
        "                props_df = pd.DataFrame(properties_data)\n",
        "                stats['properties'] = {}\n",
        "\n",
        "                for col in props_df.columns:\n",
        "                    if props_df[col].dtype in ['int64', 'float64']:\n",
        "                        col_stats = {\n",
        "                            'mean': float(props_df[col].mean()) if not props_df[col].isna().all() else None,\n",
        "                            'std': float(props_df[col].std()) if not props_df[col].isna().all() else None,\n",
        "                            'min': float(props_df[col].min()) if not props_df[col].isna().all() else None,\n",
        "                            'max': float(props_df[col].max()) if not props_df[col].isna().all() else None,\n",
        "                            'null_count': int(props_df[col].isna().sum())\n",
        "                        }\n",
        "                        stats['properties'][col] = col_stats\n",
        "\n",
        "                        # Detect statistical outliers in properties\n",
        "                        if col_stats['std'] and col_stats['std'] > 0:\n",
        "                            outliers = np.abs((props_df[col] - col_stats['mean']) / col_stats['std']) > 3\n",
        "                            if outliers.any():\n",
        "                                outlier_indices = props_df[outliers].index.tolist()\n",
        "                                issues.append(f\"Statistical outliers in property '{col}' at feature indices: {outlier_indices}\")\n",
        "                    else:\n",
        "                        # Categorical analysis\n",
        "                        unique_vals = props_df[col].nunique()\n",
        "                        null_count = props_df[col].isna().sum()\n",
        "                        stats['properties'][col] = {\n",
        "                            'unique_values': int(unique_vals),\n",
        "                            'null_count': int(null_count),\n",
        "                            'type': 'categorical'\n",
        "                        }\n",
        "\n",
        "        except Exception as e:\n",
        "            issues.append(f\"Analysis error: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            'statistics': stats,\n",
        "            'issues': issues,\n",
        "            'summary': f\"Analyzed {stats.get('total_features', 0)} features with {len(issues)} potential issues detected.\"\n",
        "        }\n",
        "\n",
        "    def _flatten_coordinates(self, coords):\n",
        "        \"\"\"Recursively flatten coordinate arrays\"\"\"\n",
        "        result = []\n",
        "\n",
        "        def _flatten(arr):\n",
        "            for item in arr:\n",
        "                if isinstance(item, (list, tuple)):\n",
        "                    if len(item) >= 2 and all(isinstance(x, (int, float)) for x in item[:2]):\n",
        "                        result.append(item[:2])  # Take only lon, lat\n",
        "                    else:\n",
        "                        _flatten(item)\n",
        "\n",
        "        _flatten(coords)\n",
        "        return result\n",
        "\n",
        "    def call(self, input_data: Dict) -> str:\n",
        "        geojson_data = input_data.get('geojson', {})\n",
        "        query = input_data.get('query', 'Analyze this GeoJSON data for anomalies')\n",
        "\n",
        "        # Perform analysis\n",
        "        analysis = self.analyze_geojson(geojson_data)\n",
        "\n",
        "        # Prepare template variables\n",
        "        template_vars = {\n",
        "            'input_str': query,\n",
        "            'stats': json.dumps(analysis['statistics'], indent=2),\n",
        "            'geojson_summary': analysis['summary'],\n",
        "            'detected_issues': '\\n'.join([f\"- {issue}\" for issue in analysis['issues']])\n",
        "        }\n",
        "\n",
        "        return self.generator.call(template_vars)\n",
        "\n",
        "    async def acall(self, input_data: Dict) -> str:\n",
        "        geojson_data = input_data.get('geojson', {})\n",
        "        query = input_data.get('query', 'Analyze this GeoJSON data for anomalies')\n",
        "\n",
        "        # Perform analysis\n",
        "        analysis = self.analyze_geojson(geojson_data)\n",
        "\n",
        "        # Prepare template variables\n",
        "        template_vars = {\n",
        "            'input_str': query,\n",
        "            'stats': json.dumps(analysis['statistics'], indent=2),\n",
        "            'geojson_summary': analysis['summary'],\n",
        "            'detected_issues': '\\n'.join([f\"- {issue}\" for issue in analysis['issues']])\n",
        "        }\n",
        "\n",
        "        return await self.generator.acall(template_vars)\n",
        "\n",
        "# Initialize the anomaly detector\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}\n",
        "\n",
        "anomaly_detector = GeoJSONAnomalyDetector(**model)\n",
        "\n",
        "# Example usage with your GeoJSON data\n",
        "# Replace 'your_geojson_data' with your actual GeoJSON data\n",
        "def detect_geojson_anomalies(geojson_data, custom_query=None):\n",
        "    \"\"\"\n",
        "    Detect anomalies in GeoJSON data\n",
        "\n",
        "    Args:\n",
        "        geojson_data: Dict containing GeoJSON data\n",
        "        custom_query: Optional custom query for specific analysis\n",
        "    \"\"\"\n",
        "    query = custom_query or \"Please analyze this GeoJSON data and identify any anomalies, outliers, or data quality issues. Focus on geometric validity, coordinate accuracy, and statistical outliers in properties.\"\n",
        "\n",
        "    input_data = {\n",
        "        'geojson': geojson_data,\n",
        "        'query': query\n",
        "    }\n",
        "\n",
        "    result = anomaly_detector(input_data)\n",
        "    display(Markdown(f\"**GeoJSON Anomaly Analysis:**\\n\\n{result.data}\"))\n",
        "    return result\n",
        "\n",
        "# Load and analyze your GeoJSON data\n",
        "print(\"Loading MunshiNagarData.geojson...\")\n",
        "\n",
        "# Option 1: Load from file\n",
        "with open('MunshiNagarData.geojson', 'r') as f:\n",
        "    your_geojson_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded GeoJSON with {len(your_geojson_data.get('features', []))} features\")\n",
        "\n",
        "# Run anomaly detection\n",
        "print(\"Running basic anomaly detection...\")\n",
        "detect_geojson_anomalies(your_geojson_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Example with custom query for geographic regions and property values\n",
        "print(\"Running detailed analysis focusing on geographic and property anomalies...\")\n",
        "detect_geojson_anomalies(\n",
        "    your_geojson_data,\n",
        "    \"Focus on detecting features that might be in wrong geographic regions or have suspicious property values. Pay special attention to any coordinates that seem out of place for Munshi Nagar area and any property values that are statistical outliers.\"\n",
        ")\n",
        "\n",
        "print(\"GeoJSON Anomaly Detection system is ready!\")\n",
        "print(\"Use detect_geojson_anomalies(your_geojson_data) to analyze your data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New code with corrected json file"
      ],
      "metadata": {
        "id": "c2OI2fEZJgnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "import pandas as pd\n",
        "import copy\n",
        "from datetime import datetime\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "!ollama pull llama3.1:8b\n",
        "clear_output()\n",
        "!pip install -U lightrag[ollama] geopy shapely\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from geopy.distance import geodesic\n",
        "from shapely.geometry import Point, Polygon, LineString, shape, mapping\n",
        "from shapely.validation import explain_validity, make_valid\n",
        "from shapely.ops import unary_union\n",
        "import time\n",
        "\n",
        "# Enhanced template for GeoJSON anomaly detection and correction\n",
        "geojson_anomaly_template = r\"\"\"<SYS>\n",
        "You are an expert geospatial data analyst specializing in detecting and correcting anomalies in GeoJSON data.\n",
        "Analyze the provided GeoJSON data and statistical analysis to identify potential anomalies and suggest corrections.\n",
        "\n",
        "Focus on detecting:\n",
        "1. Geometric anomalies (invalid coordinates, self-intersecting polygons, etc.)\n",
        "2. Statistical outliers in properties/attributes\n",
        "3. Spatial outliers (features in unexpected locations)\n",
        "4. Data consistency issues\n",
        "5. Missing or malformed data\n",
        "\n",
        "Provide specific, actionable findings with coordinates and feature IDs where applicable.\n",
        "Also suggest correction strategies for each identified issue.\n",
        "</SYS>\n",
        "\n",
        "Statistical Analysis:\n",
        "{{stats}}\n",
        "\n",
        "GeoJSON Data Summary:\n",
        "{{geojson_summary}}\n",
        "\n",
        "Detected Issues:\n",
        "{{detected_issues}}\n",
        "\n",
        "Applied Corrections:\n",
        "{{corrections_applied}}\n",
        "\n",
        "User Query: {{input_str}}\n",
        "\n",
        "Analysis and Recommendations:\"\"\"\n",
        "\n",
        "class GeoJSONAnomalyDetector(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=geojson_anomaly_template,\n",
        "        )\n",
        "        self.corrections_log = []\n",
        "\n",
        "    def analyze_geojson(self, geojson_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Perform statistical and geometric analysis on GeoJSON data\"\"\"\n",
        "        issues = []\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            features = geojson_data.get('features', [])\n",
        "            stats['total_features'] = len(features)\n",
        "\n",
        "            # Analyze geometric properties\n",
        "            coordinates_list = []\n",
        "            properties_data = []\n",
        "            geometry_types = []\n",
        "\n",
        "            for i, feature in enumerate(features):\n",
        "                # Geometry analysis\n",
        "                geometry = feature.get('geometry', {})\n",
        "                geom_type = geometry.get('type', '')\n",
        "                geometry_types.append(geom_type)\n",
        "\n",
        "                # Validate geometry using Shapely\n",
        "                try:\n",
        "                    shape_obj = shape(geometry)\n",
        "                    if not shape_obj.is_valid:\n",
        "                        issues.append(f\"Invalid geometry in feature {i}: {explain_validity(shape_obj)}\")\n",
        "                except Exception as e:\n",
        "                    issues.append(f\"Geometry parsing error in feature {i}: {str(e)}\")\n",
        "\n",
        "                # Extract coordinates for analysis\n",
        "                coords = geometry.get('coordinates', [])\n",
        "                if coords:\n",
        "                    flat_coords = self._flatten_coordinates(coords)\n",
        "                    coordinates_list.extend(flat_coords)\n",
        "\n",
        "                # Properties analysis\n",
        "                properties = feature.get('properties', {})\n",
        "                properties_data.append(properties)\n",
        "\n",
        "            # Statistical analysis of coordinates\n",
        "            if coordinates_list:\n",
        "                coords_array = np.array(coordinates_list)\n",
        "                if coords_array.shape[1] >= 2:\n",
        "                    lons = coords_array[:, 0]\n",
        "                    lats = coords_array[:, 1]\n",
        "\n",
        "                    stats['longitude'] = {\n",
        "                        'min': float(np.min(lons)),\n",
        "                        'max': float(np.max(lons)),\n",
        "                        'mean': float(np.mean(lons)),\n",
        "                        'std': float(np.std(lons))\n",
        "                    }\n",
        "                    stats['latitude'] = {\n",
        "                        'min': float(np.min(lats)),\n",
        "                        'max': float(np.max(lats)),\n",
        "                        'mean': float(np.mean(lats)),\n",
        "                        'std': float(np.std(lats))\n",
        "                    }\n",
        "\n",
        "                    # Detect coordinate outliers\n",
        "                    lon_outliers = np.abs(lons - np.mean(lons)) > 3 * np.std(lons)\n",
        "                    lat_outliers = np.abs(lats - np.mean(lats)) > 3 * np.std(lats)\n",
        "\n",
        "                    if np.any(lon_outliers):\n",
        "                        outlier_indices = np.where(lon_outliers)[0]\n",
        "                        issues.append(f\"Longitude outliers detected at indices: {outlier_indices.tolist()}\")\n",
        "\n",
        "                    if np.any(lat_outliers):\n",
        "                        outlier_indices = np.where(lat_outliers)[0]\n",
        "                        issues.append(f\"Latitude outliers detected at indices: {outlier_indices.tolist()}\")\n",
        "\n",
        "                    # Check for invalid coordinate ranges\n",
        "                    if np.any(lons < -180) or np.any(lons > 180):\n",
        "                        issues.append(\"Invalid longitude values found (outside -180 to 180 range)\")\n",
        "\n",
        "                    if np.any(lats < -90) or np.any(lats > 90):\n",
        "                        issues.append(\"Invalid latitude values found (outside -90 to 90 range)\")\n",
        "\n",
        "            # Analyze geometry types\n",
        "            stats['geometry_types'] = dict(pd.Series(geometry_types).value_counts())\n",
        "\n",
        "            # Analyze properties\n",
        "            if properties_data:\n",
        "                props_df = pd.DataFrame(properties_data)\n",
        "                stats['properties'] = {}\n",
        "\n",
        "                for col in props_df.columns:\n",
        "                    if props_df[col].dtype in ['int64', 'float64']:\n",
        "                        col_stats = {\n",
        "                            'mean': float(props_df[col].mean()) if not props_df[col].isna().all() else None,\n",
        "                            'std': float(props_df[col].std()) if not props_df[col].isna().all() else None,\n",
        "                            'min': float(props_df[col].min()) if not props_df[col].isna().all() else None,\n",
        "                            'max': float(props_df[col].max()) if not props_df[col].isna().all() else None,\n",
        "                            'null_count': int(props_df[col].isna().sum())\n",
        "                        }\n",
        "                        stats['properties'][col] = col_stats\n",
        "\n",
        "                        # Detect statistical outliers in properties\n",
        "                        if col_stats['std'] and col_stats['std'] > 0:\n",
        "                            outliers = np.abs((props_df[col] - col_stats['mean']) / col_stats['std']) > 3\n",
        "                            if outliers.any():\n",
        "                                outlier_indices = props_df[outliers].index.tolist()\n",
        "                                issues.append(f\"Statistical outliers in property '{col}' at feature indices: {outlier_indices}\")\n",
        "                    else:\n",
        "                        # Categorical analysis\n",
        "                        unique_vals = props_df[col].nunique()\n",
        "                        null_count = props_df[col].isna().sum()\n",
        "                        stats['properties'][col] = {\n",
        "                            'unique_values': int(unique_vals),\n",
        "                            'null_count': int(null_count),\n",
        "                            'type': 'categorical'\n",
        "                        }\n",
        "\n",
        "        except Exception as e:\n",
        "            issues.append(f\"Analysis error: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            'statistics': stats,\n",
        "            'issues': issues,\n",
        "            'summary': f\"Analyzed {stats.get('total_features', 0)} features with {len(issues)} potential issues detected.\"\n",
        "        }\n",
        "\n",
        "    def correct_geojson(self, geojson_data: Dict, correction_options: Dict = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Correct anomalies in GeoJSON data and return corrected version\n",
        "\n",
        "        Args:\n",
        "            geojson_data: Original GeoJSON data\n",
        "            correction_options: Dictionary of correction preferences\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing corrected GeoJSON and correction log\n",
        "        \"\"\"\n",
        "        if correction_options is None:\n",
        "            correction_options = {\n",
        "                'fix_invalid_geometries': True,\n",
        "                'remove_coordinate_outliers': True,\n",
        "                'fix_invalid_coordinates': True,\n",
        "                'remove_property_outliers': False,  # Conservative default\n",
        "                'fill_missing_properties': True,\n",
        "                'standardize_properties': True\n",
        "            }\n",
        "\n",
        "        corrected_data = copy.deepcopy(geojson_data)\n",
        "        corrections = []\n",
        "        features_to_remove = []\n",
        "\n",
        "        if 'features' not in corrected_data:\n",
        "            corrected_data['features'] = []\n",
        "\n",
        "        # Get statistics for outlier detection\n",
        "        analysis = self.analyze_geojson(geojson_data)\n",
        "        stats = analysis['statistics']\n",
        "\n",
        "        for i, feature in enumerate(corrected_data['features']):\n",
        "            feature_corrections = []\n",
        "\n",
        "            # Fix geometry issues\n",
        "            if correction_options.get('fix_invalid_geometries', True):\n",
        "                geometry = feature.get('geometry', {})\n",
        "                if geometry:\n",
        "                    try:\n",
        "                        shape_obj = shape(geometry)\n",
        "                        if not shape_obj.is_valid:\n",
        "                            # Try to fix invalid geometry\n",
        "                            fixed_shape = make_valid(shape_obj)\n",
        "                            if fixed_shape.is_valid:\n",
        "                                corrected_data['features'][i]['geometry'] = mapping(fixed_shape)\n",
        "                                feature_corrections.append(f\"Fixed invalid geometry\")\n",
        "                            else:\n",
        "                                features_to_remove.append(i)\n",
        "                                feature_corrections.append(f\"Removed unfixable invalid geometry\")\n",
        "                    except Exception as e:\n",
        "                        features_to_remove.append(i)\n",
        "                        feature_corrections.append(f\"Removed geometry due to parsing error: {str(e)}\")\n",
        "\n",
        "            # Fix coordinate issues\n",
        "            if correction_options.get('fix_invalid_coordinates', True):\n",
        "                geometry = feature.get('geometry', {})\n",
        "                coords = geometry.get('coordinates', [])\n",
        "                if coords:\n",
        "                    fixed_coords, coord_fixes = self._fix_coordinates(coords)\n",
        "                    if coord_fixes:\n",
        "                        corrected_data['features'][i]['geometry']['coordinates'] = fixed_coords\n",
        "                        feature_corrections.extend(coord_fixes)\n",
        "\n",
        "            # Handle coordinate outliers\n",
        "            if correction_options.get('remove_coordinate_outliers', True) and 'longitude' in stats and 'latitude' in stats:\n",
        "                geometry = feature.get('geometry', {})\n",
        "                if geometry and self._is_coordinate_outlier(geometry, stats):\n",
        "                    features_to_remove.append(i)\n",
        "                    feature_corrections.append(\"Removed feature with outlier coordinates\")\n",
        "\n",
        "            # Fix property issues\n",
        "            properties = feature.get('properties', {})\n",
        "            if properties and correction_options.get('standardize_properties', True):\n",
        "                fixed_props, prop_fixes = self._fix_properties(properties, stats.get('properties', {}), correction_options)\n",
        "                if prop_fixes:\n",
        "                    corrected_data['features'][i]['properties'] = fixed_props\n",
        "                    feature_corrections.extend(prop_fixes)\n",
        "\n",
        "            if feature_corrections:\n",
        "                corrections.append(f\"Feature {i}: {'; '.join(feature_corrections)}\")\n",
        "\n",
        "        # Remove flagged features (in reverse order to maintain indices)\n",
        "        for idx in sorted(set(features_to_remove), reverse=True):\n",
        "            del corrected_data['features'][idx]\n",
        "            corrections.append(f\"Removed feature {idx}\")\n",
        "\n",
        "        # Add metadata about corrections\n",
        "        if 'properties' not in corrected_data:\n",
        "            corrected_data['properties'] = {}\n",
        "\n",
        "        corrected_data['properties']['correction_metadata'] = {\n",
        "            'corrected_at': datetime.now().isoformat(),\n",
        "            'original_feature_count': len(geojson_data.get('features', [])),\n",
        "            'corrected_feature_count': len(corrected_data.get('features', [])),\n",
        "            'corrections_applied': len(corrections),\n",
        "            'correction_options': correction_options\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'corrected_geojson': corrected_data,\n",
        "            'corrections_log': corrections,\n",
        "            'correction_summary': f\"Applied {len(corrections)} corrections, removed {len(features_to_remove)} features\"\n",
        "        }\n",
        "\n",
        "    def _fix_coordinates(self, coords) -> Tuple[List, List]:\n",
        "        \"\"\"Fix coordinate arrays recursively\"\"\"\n",
        "        fixes = []\n",
        "\n",
        "        def fix_coord_array(arr):\n",
        "            if not isinstance(arr, (list, tuple)):\n",
        "                return arr\n",
        "\n",
        "            # If this looks like a coordinate pair [lon, lat]\n",
        "            if len(arr) >= 2 and all(isinstance(x, (int, float)) for x in arr[:2]):\n",
        "                lon, lat = arr[0], arr[1]\n",
        "                fixed_lon, fixed_lat = lon, lat\n",
        "\n",
        "                # Fix longitude range\n",
        "                if lon < -180:\n",
        "                    fixed_lon = -180\n",
        "                    fixes.append(f\"Clamped longitude {lon} to -180\")\n",
        "                elif lon > 180:\n",
        "                    fixed_lon = 180\n",
        "                    fixes.append(f\"Clamped longitude {lon} to 180\")\n",
        "\n",
        "                # Fix latitude range\n",
        "                if lat < -90:\n",
        "                    fixed_lat = -90\n",
        "                    fixes.append(f\"Clamped latitude {lat} to -90\")\n",
        "                elif lat > 90:\n",
        "                    fixed_lat = 90\n",
        "                    fixes.append(f\"Clamped latitude {lat} to 90\")\n",
        "\n",
        "                result = [fixed_lon, fixed_lat]\n",
        "                if len(arr) > 2:\n",
        "                    result.extend(arr[2:])  # Preserve additional dimensions\n",
        "                return result\n",
        "            else:\n",
        "                # Recursively fix nested arrays\n",
        "                return [fix_coord_array(item) for item in arr]\n",
        "\n",
        "        fixed_coords = fix_coord_array(coords)\n",
        "        return fixed_coords, fixes\n",
        "\n",
        "    def _is_coordinate_outlier(self, geometry: Dict, stats: Dict) -> bool:\n",
        "        \"\"\"Check if geometry contains outlier coordinates\"\"\"\n",
        "        try:\n",
        "            coords = geometry.get('coordinates', [])\n",
        "            flat_coords = self._flatten_coordinates(coords)\n",
        "\n",
        "            if not flat_coords or 'longitude' not in stats or 'latitude' not in stats:\n",
        "                return False\n",
        "\n",
        "            lon_mean, lon_std = stats['longitude']['mean'], stats['longitude']['std']\n",
        "            lat_mean, lat_std = stats['latitude']['mean'], stats['latitude']['std']\n",
        "\n",
        "            for coord in flat_coords:\n",
        "                if len(coord) >= 2:\n",
        "                    lon, lat = coord[0], coord[1]\n",
        "\n",
        "                    # Check if coordinate is more than 3 standard deviations away\n",
        "                    if (abs(lon - lon_mean) > 3 * lon_std or\n",
        "                        abs(lat - lat_mean) > 3 * lat_std):\n",
        "                        return True\n",
        "\n",
        "            return False\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _fix_properties(self, properties: Dict, prop_stats: Dict, options: Dict) -> Tuple[Dict, List]:\n",
        "        \"\"\"Fix property issues\"\"\"\n",
        "        fixed_props = copy.deepcopy(properties)\n",
        "        fixes = []\n",
        "\n",
        "        for key, value in properties.items():\n",
        "            if key in prop_stats:\n",
        "                stat_info = prop_stats[key]\n",
        "\n",
        "                # Handle missing values\n",
        "                if pd.isna(value) and options.get('fill_missing_properties', True):\n",
        "                    if stat_info.get('type') != 'categorical' and stat_info.get('mean') is not None:\n",
        "                        fixed_props[key] = stat_info['mean']\n",
        "                        fixes.append(f\"Filled missing {key} with mean value\")\n",
        "                    elif stat_info.get('type') == 'categorical':\n",
        "                        fixed_props[key] = \"Unknown\"\n",
        "                        fixes.append(f\"Filled missing {key} with 'Unknown'\")\n",
        "\n",
        "                # Handle outliers in numeric properties\n",
        "                elif (options.get('remove_property_outliers', False) and\n",
        "                      stat_info.get('type') != 'categorical' and\n",
        "                      stat_info.get('std') and stat_info.get('mean') is not None):\n",
        "\n",
        "                    if isinstance(value, (int, float)):\n",
        "                        z_score = abs((value - stat_info['mean']) / stat_info['std'])\n",
        "                        if z_score > 3:\n",
        "                            # Replace with median or mean\n",
        "                            fixed_props[key] = stat_info['mean']\n",
        "                            fixes.append(f\"Replaced outlier {key} value {value} with mean\")\n",
        "\n",
        "        return fixed_props, fixes\n",
        "\n",
        "    def _flatten_coordinates(self, coords):\n",
        "        \"\"\"Recursively flatten coordinate arrays\"\"\"\n",
        "        result = []\n",
        "\n",
        "        def _flatten(arr):\n",
        "            for item in arr:\n",
        "                if isinstance(item, (list, tuple)):\n",
        "                    if len(item) >= 2 and all(isinstance(x, (int, float)) for x in item[:2]):\n",
        "                        result.append(item[:2])  # Take only lon, lat\n",
        "                    else:\n",
        "                        _flatten(item)\n",
        "\n",
        "        _flatten(coords)\n",
        "        return result\n",
        "\n",
        "    def call(self, input_data: Dict) -> str:\n",
        "        geojson_data = input_data.get('geojson', {})\n",
        "        query = input_data.get('query', 'Analyze this GeoJSON data for anomalies')\n",
        "\n",
        "        # Perform analysis\n",
        "        analysis = self.analyze_geojson(geojson_data)\n",
        "\n",
        "        # Apply corrections if requested\n",
        "        correction_result = None\n",
        "        if input_data.get('apply_corrections', False):\n",
        "            correction_options = input_data.get('correction_options', {})\n",
        "            correction_result = self.correct_geojson(geojson_data, correction_options)\n",
        "\n",
        "        # Prepare template variables\n",
        "        template_vars = {\n",
        "            'input_str': query,\n",
        "            'stats': json.dumps(analysis['statistics'], indent=2),\n",
        "            'geojson_summary': analysis['summary'],\n",
        "            'detected_issues': '\\n'.join([f\"- {issue}\" for issue in analysis['issues']]),\n",
        "            'corrections_applied': (\n",
        "                '\\n'.join([f\"- {correction}\" for correction in correction_result['corrections_log']])\n",
        "                if correction_result else \"No corrections applied\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return self.generator.call(template_vars)\n",
        "\n",
        "    async def acall(self, input_data: Dict) -> str:\n",
        "        geojson_data = input_data.get('geojson', {})\n",
        "        query = input_data.get('query', 'Analyze this GeoJSON data for anomalies')\n",
        "\n",
        "        # Perform analysis\n",
        "        analysis = self.analyze_geojson(geojson_data)\n",
        "\n",
        "        # Apply corrections if requested\n",
        "        correction_result = None\n",
        "        if input_data.get('apply_corrections', False):\n",
        "            correction_options = input_data.get('correction_options', {})\n",
        "            correction_result = self.correct_geojson(geojson_data, correction_options)\n",
        "\n",
        "        # Prepare template variables\n",
        "        template_vars = {\n",
        "            'input_str': query,\n",
        "            'stats': json.dumps(analysis['statistics'], indent=2),\n",
        "            'geojson_summary': analysis['summary'],\n",
        "            'detected_issues': '\\n'.join([f\"- {issue}\" for issue in analysis['issues']]),\n",
        "            'corrections_applied': (\n",
        "                '\\n'.join([f\"- {correction}\" for correction in correction_result['corrections_log']])\n",
        "                if correction_result else \"No corrections applied\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return await self.generator.acall(template_vars)\n",
        "\n",
        "# Initialize the anomaly detector\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}\n",
        "\n",
        "anomaly_detector = GeoJSONAnomalyDetector(**model)\n",
        "\n",
        "def detect_and_correct_geojson_anomalies(geojson_data, custom_query=None, apply_corrections=True, correction_options=None):\n",
        "    \"\"\"\n",
        "    Detect and optionally correct anomalies in GeoJSON data\n",
        "\n",
        "    Args:\n",
        "        geojson_data: Dict containing GeoJSON data\n",
        "        custom_query: Optional custom query for specific analysis\n",
        "        apply_corrections: Whether to apply automatic corrections\n",
        "        correction_options: Dictionary of correction preferences\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing analysis results and corrected data (if requested)\n",
        "    \"\"\"\n",
        "    if correction_options is None:\n",
        "        correction_options = {\n",
        "            'fix_invalid_geometries': True,\n",
        "            'remove_coordinate_outliers': True,\n",
        "            'fix_invalid_coordinates': True,\n",
        "            'remove_property_outliers': False,\n",
        "            'fill_missing_properties': True,\n",
        "            'standardize_properties': True\n",
        "        }\n",
        "\n",
        "    query = custom_query or \"Please analyze this GeoJSON data and identify any anomalies, outliers, or data quality issues. Focus on geometric validity, coordinate accuracy, and statistical outliers in properties.\"\n",
        "\n",
        "    input_data = {\n",
        "        'geojson': geojson_data,\n",
        "        'query': query,\n",
        "        'apply_corrections': apply_corrections,\n",
        "        'correction_options': correction_options\n",
        "    }\n",
        "\n",
        "    # Get AI analysis\n",
        "    result = anomaly_detector(input_data)\n",
        "    display(Markdown(f\"**GeoJSON Anomaly Analysis:**\\n\\n{result.data}\"))\n",
        "\n",
        "    # Get corrected data if requested\n",
        "    corrected_result = None\n",
        "    if apply_corrections:\n",
        "        corrected_result = anomaly_detector.correct_geojson(geojson_data, correction_options)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"CORRECTION SUMMARY:\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Original features: {len(geojson_data.get('features', []))}\")\n",
        "        print(f\"Corrected features: {len(corrected_result['corrected_geojson'].get('features', []))}\")\n",
        "        print(f\"Corrections applied: {len(corrected_result['corrections_log'])}\")\n",
        "\n",
        "        if corrected_result['corrections_log']:\n",
        "            print(f\"\\nDetailed corrections:\")\n",
        "            for i, correction in enumerate(corrected_result['corrections_log'][:10], 1):  # Show first 10\n",
        "                print(f\"{i}. {correction}\")\n",
        "\n",
        "            if len(corrected_result['corrections_log']) > 10:\n",
        "                print(f\"... and {len(corrected_result['corrections_log']) - 10} more corrections\")\n",
        "\n",
        "    return {\n",
        "        'analysis': result,\n",
        "        'corrected_data': corrected_result,\n",
        "        'original_data': geojson_data\n",
        "    }\n",
        "\n",
        "def save_corrected_geojson(correction_result, filename=\"corrected_data.geojson\"):\n",
        "    \"\"\"Save corrected GeoJSON data to file\"\"\"\n",
        "    if correction_result and 'corrected_data' in correction_result and correction_result['corrected_data']:\n",
        "        corrected_geojson = correction_result['corrected_data']['corrected_geojson']\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(corrected_geojson, f, indent=2)\n",
        "\n",
        "        print(f\"Corrected GeoJSON saved to: {filename}\")\n",
        "        return filename\n",
        "    else:\n",
        "        print(\"No corrected data available to save\")\n",
        "        return None\n",
        "\n",
        "# Load and analyze your GeoJSON data\n",
        "print(\"Loading MunshiNagarData.geojson...\")\n",
        "\n",
        "# Load from file\n",
        "with open('MunshiNagarData.geojson', 'r') as f:\n",
        "    your_geojson_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded GeoJSON with {len(your_geojson_data.get('features', []))} features\")\n",
        "\n",
        "# Run anomaly detection and correction\n",
        "print(\"Running comprehensive anomaly detection and correction...\")\n",
        "result = detect_and_correct_geojson_anomalies(\n",
        "    your_geojson_data,\n",
        "    custom_query=\"Analyze this Munshi Nagar GeoJSON data for any anomalies and apply appropriate corrections. Focus on geometric validity, coordinate accuracy, and data consistency.\",\n",
        "    apply_corrections=True\n",
        ")\n",
        "\n",
        "# Save corrected data\n",
        "if result['corrected_data']:\n",
        "    corrected_filename = save_corrected_geojson(result, \"MunshiNagarData_corrected.geojson\")\n",
        "    print(f\"\\nCorrected GeoJSON data has been saved as: {corrected_filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Enhanced GeoJSON Anomaly Detection and Correction system is ready!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"- detect_and_correct_geojson_anomalies(data, query, apply_corrections, options)\")\n",
        "print(\"- save_corrected_geojson(result, filename)\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mDql04X8JkQb",
        "outputId": "b4c93f69-e50c-4c91-f76a-15fe9a5ca258"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Collecting shapely\n",
            "  Downloading shapely-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lightrag[ollama]\n",
            "  Downloading lightrag-0.1.0b6-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting backoff<3.0.0,>=2.2.1 (from lightrag[ollama])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (3.1.6)\n",
            "Collecting jsonlines<5.0.0,>=4.0.0 (from lightrag[ollama])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Collecting numpy<2.0.0,>=1.26.4 (from lightrag[ollama])\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ollama<0.3.0,>=0.2.1 (from lightrag[ollama])\n",
            "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from lightrag[ollama])\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from lightrag[ollama])\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from lightrag[ollama]) (4.67.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (3.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (25.3.0)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama<0.3.0,>=0.2.1->lightrag[ollama])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.13.2)\n",
            "Downloading shapely-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightrag-0.1.0b6-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, numpy, jsonlines, backoff, tiktoken, shapely, httpx, ollama, lightrag\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.9.0\n",
            "    Uninstalling tiktoken-0.9.0:\n",
            "      Successfully uninstalled tiktoken-0.9.0\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.1.0\n",
            "    Uninstalling shapely-2.1.0:\n",
            "      Successfully uninstalled shapely-2.1.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.15.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 httpx-0.27.2 jsonlines-4.0.0 lightrag-0.1.0b6 numpy-1.26.4 ollama-0.2.1 python-dotenv-1.1.0 shapely-2.1.1 tiktoken-0.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off call(...) for 0.1s (ollama._types.ResponseError: model 'llama3.1:8b' not found)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MunshiNagarData.geojson...\n",
            "Loaded GeoJSON with 212 features\n",
            "Running comprehensive anomaly detection and correction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off call(...) for 0.2s (ollama._types.ResponseError: model 'llama3.1:8b' not found)\n",
            "INFO:backoff:Backing off call(...) for 2.3s (ollama._types.ResponseError: model 'llama3.1:8b' not found)\n",
            "INFO:backoff:Backing off call(...) for 2.4s (ollama._types.ResponseError: model 'llama3.1:8b' not found)\n",
            "ERROR:backoff:Giving up call(...) after 5 tries (ollama._types.ResponseError: model 'llama3.1:8b' not found)\n",
            "ERROR:lightrag.core.generator:Error calling the model: model 'llama3.1:8b' not found\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**GeoJSON Anomaly Analysis:**\n\nNone"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CORRECTION SUMMARY:\n",
            "============================================================\n",
            "Original features: 212\n",
            "Corrected features: 210\n",
            "Corrections applied: 5\n",
            "\n",
            "Detailed corrections:\n",
            "1. Feature 0: Removed geometry due to parsing error: 'float' object is not iterable\n",
            "2. Feature 1: Fixed invalid geometry\n",
            "3. Feature 2: Fixed invalid geometry; Clamped latitude 90.65 to 90; Removed feature with outlier coordinates\n",
            "4. Removed feature 2\n",
            "5. Removed feature 0\n",
            "Corrected GeoJSON saved to: MunshiNagarData_corrected.geojson\n",
            "\n",
            "Corrected GeoJSON data has been saved as: MunshiNagarData_corrected.geojson\n",
            "\n",
            "================================================================================\n",
            "Enhanced GeoJSON Anomaly Detection and Correction system is ready!\n",
            "Available functions:\n",
            "- detect_and_correct_geojson_anomalies(data, query, apply_corrections, options)\n",
            "- save_corrected_geojson(result, filename)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZRL2yXtNw5_",
        "outputId": "17bc1d03-0326-481b-cd5c-b3e903e7c1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.4.26)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    }
  ]
}